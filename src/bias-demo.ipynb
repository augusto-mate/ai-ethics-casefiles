{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f697aff",
   "metadata": {},
   "source": [
    "# Bias Demonstration: Gender and Algorithmic Decisions\n",
    "\n",
    "This notebook illustrates how even simple models trained on toy datasets can reflect or amplify gender bias. It is intended as an educational tool to understand fairness in AI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a77f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed687f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset: gender and test score\n",
    "data = pd.DataFrame({\n",
    "    'gender': ['male', 'female'] * 10,\n",
    "    'score': [82, 76, 85, 65, 90, 60, 88, 58, 84, 59, 87, 61, 83, 62, 91, 57, 89, 55, 86, 56],\n",
    "    'admitted': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "})\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f830ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='gender', y='score', data=data)\n",
    "plt.title('Score Distribution by Gender')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c54b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(data, drop_first=True)\n",
    "X = data_encoded[['score', 'gender_male']]\n",
    "y = data_encoded['admitted']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "})\n",
    "coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e0194",
   "metadata": {},
   "source": [
    "## Ethical Observations\n",
    "\n",
    "- The gender feature is directly contributing to the prediction.\n",
    "- This is a toy dataset, but in real settings, such encoding may reinforce structural bias.\n",
    "- Removing or masking sensitive attributes may not be sufficient — proxy variables (e.g., ZIP code) can still leak bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfafcc",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- This dataset is synthetic and overly simplified.\n",
    "- Results are for educational clarity only, and should not be interpreted as representative of real-world outcomes.\n",
    "- Fairness is a context-dependent issue — what is fair in one domain may not be in another.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
